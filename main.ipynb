{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc  # Garbage Collector interface\n",
    "import math\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishment of Workforce Training Vocabulary Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path containing your text files\n",
    "folder_path = \"E:\\\\workforce\\\\workforce_training_ds\"\n",
    "\n",
    "# Initialize a dictionary to store filtered words for each file\n",
    "filtered_words_by_file = {}\n",
    "\n",
    "# Define a list of English letters\n",
    "eng_l = ['a', 'A', 'b', 'B', 'c', 'C', 'd', 'D', 'e', 'E', 'f', 'F', 'g', 'G', 'h', 'H', 'i', 'I', 'j', 'J', 'k', 'K', 'l', 'L', 'm', 'M', 'n', 'N', 'o', 'O', 'p', 'P', 'q', 'Q', 'r', 'R', 's', 'S', 't', 'T', 'u', 'U', 'v', 'V', 'w', 'W', 'x', 'X', 'y', 'Y', 'z', 'Z']\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        # Open and read the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        \n",
    "        # Tokenize and preprocess the text\n",
    "        words = word_tokenize(text.lower())\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        print(len(filtered_words))\n",
    "        # Create a list to store meaningful words\n",
    "        wanted = []\n",
    "        for word in filtered_words:\n",
    "            if word[0] in eng_l:\n",
    "                wanted.append(word)\n",
    "        \n",
    "        # Store the filtered words for the current file in the dictionary\n",
    "        filtered_words_by_file[filename] = wanted\n",
    "\n",
    "# Now you have a dictionary where each key is a filename, and the value is a list of filtered words for that file\n",
    "for filename, words in filtered_words_by_file.items():\n",
    "    print(f\"File: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary holding the specific number of words you want for each file\n",
    "top_n_by_file = {\n",
    "    'eco1.txt': 30000,\n",
    "    'eco2.txt': 30000,\n",
    "    'eco3.txt': 30000,\n",
    "    'eco4.txt': 30000,\n",
    "    'eco5.txt': 30000\n",
    "}\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Initialize a dictionary to store TF-IDF results for each file\n",
    "tfidf_results = {}\n",
    "words_with_positive_tfidf = {}\n",
    "all_words_results = pd.DataFrame()\n",
    "\n",
    "for filename, words in filtered_words_by_file.items():\n",
    "    # Combine the filtered words into a single string\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    # Fit and transform the TF-IDF vectorizer on the document\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([document])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "    # Sort words by their TF-IDF scores\n",
    "    sorted_tfidf_df = tfidf_df.T.sort_values(by=0, ascending=False)\n",
    "\n",
    "    # Store the sorted TF-IDF DataFrame in the results dictionary\n",
    "    tfidf_results[filename] = sorted_tfidf_df\n",
    "\n",
    "    # Print the top N words with the highest TF-IDF scores\n",
    "    top_n = top_n_by_file.get(filename, 2000)\n",
    "    print(f\"Top {top_n} words in {filename}:\")\n",
    "    print(sorted_tfidf_df.head(top_n))\n",
    "\n",
    "    # Retrieve the top words\n",
    "    top_words = sorted_tfidf_df.head(top_n).reset_index()['index']  # Only retrieve the words, not the scores\n",
    "    \n",
    "    # Convert the series to a DataFrame\n",
    "    top_words_df = top_words.to_frame(name='Word')\n",
    "\n",
    "    # Append these words in the all_words_results DataFrame\n",
    "    all_words_results = all_words_results.append(top_words_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The code uses spaCy to process the tokens and identify the positions of any synonyms. It then counts the labor-related words that appear within 10 words of any synonym, and calculates the labor risk indicator as the ratio of matched labor words to the total word count in the file.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the political bigrams directly into a set\n",
    "with open('labor_list_words.csv', 'r', encoding='utf-8') as file:\n",
    "    labor_word_set = set(file.read().splitlines())\n",
    "\n",
    "# Load synonyms\n",
    "with open('synonyms.txt', 'r', encoding='utf-8') as file:\n",
    "    synonyms = file.read().splitlines()\n",
    "\n",
    "# Load the spaCy model with disabled components for efficiency\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"ner\", \"parser\"])\n",
    "\n",
    "def compute_prisk_for_file(filepath, synonyms, labor_word_set):\n",
    "    # Initialize the counter for matched words\n",
    "    matched_words_count = 0\n",
    "    total_word_count = 0\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # Process the file line by line\n",
    "        for line in file:\n",
    "            # Clean the text by removing punctuation\n",
    "            clean_line = re.sub(r'[^\\w\\s]', '', line)\n",
    "\n",
    "            # Tokenize the cleaned line\n",
    "            words = clean_line.split()\n",
    "            total_word_count += len(words)\n",
    "\n",
    "            # Process the tokens using spaCy\n",
    "            doc = nlp(clean_line)\n",
    "\n",
    "            # Find the positions of synonyms in the tokens\n",
    "            positions = [token.i for token in doc if token.text in synonyms]\n",
    "\n",
    "            # Match the labor words in proximity to the synonyms\n",
    "            matched_words_count += sum(1 for i, word in enumerate(words) if word in labor_word_set and any(abs(i - pos) < 10 for pos in positions))\n",
    "\n",
    "    # Clear the memory for unreferenced objects\n",
    "    gc.collect()\n",
    "\n",
    "    # Calculate the risk indicator\n",
    "    if total_word_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return matched_words_count / total_word_count\n",
    "\n",
    "def process_year(year, root, synonyms, labor_word_set):\n",
    "    # Define the path to the transcripts based on the year\n",
    "    transcripts_path = os.path.join(root, str(year))\n",
    "    results = []\n",
    "\n",
    "    # Process each transcript file\n",
    "    for filename in os.listdir(transcripts_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(transcripts_path, filename)\n",
    "            Lrisk = compute_prisk_for_file(filepath, synonyms, labor_word_set)\n",
    "            results.append((filename, Lrisk))\n",
    "            print(f\"Filename: {filename}\")\n",
    "            print(f\"Lrisk: {Lrisk}\")\n",
    "            print(\"--------\")\n",
    "\n",
    "    # Create a DataFrame from the results and save it as a CSV file\n",
    "    df = pd.DataFrame(results, columns=['Filename', 'Lrisk'])\n",
    "    df.to_csv(f\"Lrisk_{year}.csv\", index=False)\n",
    "    print(f\"Results for {year} saved to Lrisk_{year}.csv\")\n",
    "\n",
    "# Set the root directory for the transcripts\n",
    "root = 'E:\\\\workforce\\\\'\n",
    "\n",
    "# Process the transcripts for each year\n",
    "for year in range(2001, 2024):\n",
    "    process_year(year, root, synonyms, labor_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on [Deloitte's labor-related keywords](https://action.deloitte.com/insight/3087/what-is-workforce-risk-its-broader-than-you-think), vectorize the original labor_list_words and calculate cosine similarity, categorizing them into highly relevant (score 5) and generally relevant (score 3).\n",
    "\n",
    "`1. exact_match_threshold and partial_match_threshold: These are similarity thresholds used to classify words. If the similarity between two words is higher than the exact_match_threshold, the word is classified as a 5. If the similarity falls between the partial_match_threshold and the exact_match_threshold, it is classified as a 3. Otherwise, it is scored 0.`\n",
    "\n",
    "`2. Classifying words: Using a for loop to iterate through each word in words_to_classify, the similarity with all words in labor_words is calculated. Based on the similarity values, words are classified into different scores. The word and its score are then added to the classified_words list.`\n",
    "\n",
    "`3. classified_words_df: Convert classified_words into a DataFrame and name the columns 'Word' and 'Score' for better visualization and analysis.`\n",
    "\n",
    "`4. score_distribution: Analyze the score distribution of the classified words by calculating the number of words for each score.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labor words:deloitte\n",
    "labor_words = [\n",
    "    \"skills\", \"talent\", \"availability\", \"ability\", \"address\", \"changing\", \n",
    "    \"workforce\", \"expectations\", \"disruptions\", \"location\", \n",
    "    \"workforce-related\", \"regulations\", \"compliance\", \"amplified\", \"voice\", \n",
    "    \"individual\", \"workers\", \"esg\", \"diversity,\", \"equity,\", \"inclusion\", \n",
    "    \"workforce\", \"trust,\", \"purpose,\", \"mission\", \"ability\", \"plan\", \"deploy\", \n",
    "    \"workers\", \"evolving\", \"organizational\", \"needs\", \"well-being,\", \n",
    "    \"compensation,\", \"rewards\", \"data\", \"technology\", \"responsible\", \"use\", \n",
    "    \"workforce\", \"artificial\", \"intelligence,\", \"cybersecurity,\", \"data\", \n",
    "    \"privacy\", \"industries\", \"industry\", \"unemployed\", \"layoffs\", \"bureau\", \n",
    "    \"workers\", \"employees\", \"employee\", \"payroll\", \"unemployment\", \"employer\", \n",
    "    \"quit\", \"work\", \"opportunities\", \"occupations\", \"employing\", \"jobseekers\",\n",
    "    \"labor\", \"openings\", \"rate\", \"hires\", \"job\", \"employment\", \"jobs\",\"abilities\",\"earnings\",\"employers\",\"employ\",\"employability\",\"employable\",\"employed\",\"employs\",\"expects\",\"expecting\",'expected',\"expectancy\",\"expectancies\",\"expectations\",\"expectation\",\"hire\",\"hired\",\n",
    "    \"hiring\",\"include\",\"included\",\"includes\",\"including\",\"inclusive\",\"individuals\",\"industrial\",\"intelligent\",\"intellectual\",\"intelligences\",\"jobs\",\"jobsecurity\",\"jobseeker\",\"jobseeking\",\"labour\",\"labors\",\"labourmarket\",\"layoff\",\"wages\",\"wage\",\n",
    "    \"workplace\",\"workplaces\",\"works\",\"worksite\",\"worksites\",\"workstation\",\"workstations\"\n",
    "]\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = \"E:\\\\workforce\\\\labor_list_words.csv\"\n",
    "labor_list_words_df = pd.read_csv(file_path)\n",
    "labor_list_words_df = labor_list_words_df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words from the uploaded file\n",
    "words_to_classify = labor_list_words_df['Word'].tolist()\n",
    "\n",
    "# Combine both lists for vectorization\n",
    "combined_words = labor_words + words_to_classify\n",
    "\n",
    "# Vectorize the words using CountVectorizer\n",
    "vectorizer = CountVectorizer().fit(combined_words)\n",
    "word_vectors = vectorizer.transform(combined_words)\n",
    "\n",
    "# Calculate cosine similarity between each word in labor_words and words_to_classify\n",
    "similarity_matrix = cosine_similarity(word_vectors[:len(labor_words)], word_vectors[len(labor_words):])\n",
    "\n",
    "# Define the thresholds for classification\n",
    "exact_match_threshold = 0.8  # High similarity\n",
    "partial_match_threshold = 0.4 # Moderate similarity\n",
    "\n",
    "# Classify the words\n",
    "classified_words = []\n",
    "for i, word in enumerate(words_to_classify):\n",
    "    max_similarity = np.max(similarity_matrix[:, i])\n",
    "    if max_similarity >= exact_match_threshold:\n",
    "        score = 5\n",
    "    elif max_similarity >= partial_match_threshold:\n",
    "        score = 3\n",
    "    else:\n",
    "        score = 0\n",
    "    classified_words.append((word, score))\n",
    "\n",
    "# Convert the results to a DataFrame for better visualization\n",
    "classified_words_df = pd.DataFrame(classified_words, columns=['Word', 'Score'])\n",
    "\n",
    "# Analyzing the distribution of scores in the adjusted classification\n",
    "score_distribution = classified_words_df['Score'].value_counts().sort_index()\n",
    "\n",
    "# Display the score distribution\n",
    "score_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. Create an indirect word list comprising words not categorized as highly or generally relevant, indicating low relevance or irrelevance, derived from the labor_word_list.`\n",
    "\n",
    "`2. Vectorize these indirect words and calculate cosine similarity, applying the same logic as above to categorize them into scores of 0 and 1 based on their similarity scores.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided list of indirect words\n",
    "indirect_words = [\n",
    "    \"jolts\", \"products\", \"manufacturing\", \"goods\", \"percent\", \"estimates\", \"trends\", \"united\",\n",
    "    \"seasonally\", \"adjusted\", \"fill\", \"level\", \"market\", \"durable\", \"average\", \"series\",\n",
    "    \"nondurable\", \"total\", \"persons\", \"experienced\", \"discharges\", \"turnover\", \"recession\", \"annual\",\n",
    "    \"million\", \"levels\", \"demand\", \"peak\", \"number\", \"statistics\", \"interactive\", \"categories\",\n",
    "    \"values\", \"ratio\", \"system\", \"thousands\", \"business\", \"regional\", \"region\", \"people\",\n",
    "    \"machinery\", \"equipment\", \"information\", \"growth\", \"released\", \"cyclical\", \"services\", \"component\",\n",
    "    \"measures\", \"nation\", \"supply\", \"economies\", \"decisions\", \"structure\",\"development\",\"develops\",\"device\",\"devices\",\"devise\",\"dismal\",\"dismiss\",\"dismissal\",\"dismissals\",\"dismissed\",\"dismissing\",\n",
    "    \"disrupted\",\"disrupting\",\"disruption\",\"disruptions\",\"disruptive\",\"disrupts\",\"economic\",\"economically\",\"economical\",\"economics\",\"economist\",\"economy\",\"eliminate\",\"eliminated\",\"eliminating\",\"elimination\",\"entrepreneur\",\"entrepreneurial\",\"entrepreneurs\",\"entrepreneurship\",\"estimate\",\"estimated\",\"estimating\",\"estimation\",\"estimators\",\"experience\",\"experiences\",\"experiencing\",\"experiential\",\"experiment\",\"experiments\",\"filled\",\"filling\",\"grow\",\"growing\",\"grown\",\"grows\",\n",
    "    \"informative\",\"leverage\",\"leveraged\",\"leverages\",\"leveraging\",\"machine\",\"machines\",\"markets\",\"measured\",\"measurement\",\"measurements\",\n",
    "    \"warehouse\",\"warehouses\",\"welfare\"\n",
    "]\n",
    "\n",
    "# Filter out words from classified_words_df where score is not 0\n",
    "words_to_reclassify = classified_words_df[classified_words_df['Score'] == 0]['Word'].tolist()\n",
    "\n",
    "# Combine indirect words with words to reclassify for vectorization\n",
    "combined_words_for_reclassification = indirect_words + words_to_reclassify\n",
    "\n",
    "# Vectorize the words using CountVectorizer\n",
    "vectorizer_reclassify = CountVectorizer().fit(combined_words_for_reclassification)\n",
    "word_vectors_reclassify = vectorizer_reclassify.transform(combined_words_for_reclassification)\n",
    "\n",
    "# Calculate cosine similarity between each word in indirect_words and words_to_reclassify\n",
    "similarity_matrix_reclassify = cosine_similarity(word_vectors_reclassify[:len(indirect_words)], word_vectors_reclassify[len(indirect_words):])\n",
    "\n",
    "# Define the thresholds for reclassification\n",
    "indirect_match_threshold = 0.8  # High similarity for indirect match\n",
    "no_relevance_threshold = 0.4    # Lower bound for no relevance\n",
    "\n",
    "# Reclassify the words\n",
    "for i, word in enumerate(words_to_reclassify):\n",
    "    max_similarity = np.max(similarity_matrix_reclassify[:, i])\n",
    "    if max_similarity >= indirect_match_threshold:\n",
    "        score = 1  # Indirect Match\n",
    "    elif max_similarity < no_relevance_threshold:\n",
    "        score = 0  # No Relevance\n",
    "    else:\n",
    "        # If a word had a score of 0 and doesn't meet the criteria for indirect match or no relevance,\n",
    "        # keep its original score (which is 0)\n",
    "        continue\n",
    "\n",
    "    # Update the score in the original dataframe\n",
    "    classified_words_df.loc[classified_words_df['Word'] == word, 'Score'] = score\n",
    "\n",
    "# Display the updated score distribution\n",
    "updated_score_distribution = classified_words_df['Score'].value_counts().sort_index()\n",
    "updated_score_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labor words with their scores into a dictionary\n",
    "with open('deloitte.csv', 'r', encoding='utf-8') as file:\n",
    "    labor_word_dict = {line.split(',')[0]: int(line.split(',')[1].strip()) for line in file.readlines()[1:]}\n",
    "\n",
    "# Load synonyms\n",
    "with open('synonyms.txt', 'r', encoding='utf-8') as file:\n",
    "    synonyms = file.read().splitlines()\n",
    "synonyms = list(dict.fromkeys(synonyms))\n",
    "\n",
    "# Load the spaCy model with disabled components for efficiency\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"ner\", \"parser\"])\n",
    "\n",
    "def compute_prisk_for_file(filepath, synonyms, labor_word_dict):\n",
    "    # Initialize the counter for matched words\n",
    "    matched_words_score = 0\n",
    "    total_word_count = 0\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        # Process the file line by line\n",
    "        for line in file:\n",
    "            # Clean the text by removing punctuation\n",
    "            clean_line = re.sub(r'[^\\w\\s]', '', line)\n",
    "\n",
    "            # Tokenize the cleaned line\n",
    "            words = clean_line.split()\n",
    "            total_word_count += len(words)\n",
    "\n",
    "            # Process the tokens using spaCy\n",
    "            doc = nlp(clean_line)\n",
    "\n",
    "            # Find the positions of synonyms in the tokens\n",
    "            positions = [token.i for token in doc if token.text in synonyms]\n",
    "\n",
    "            # Match the labor words in proximity to the synonyms and add their scores\n",
    "            for i, word in enumerate(words):\n",
    "                if word in labor_word_dict and any(abs(i - pos) < 10 for pos in positions):\n",
    "                    matched_words_score += labor_word_dict[word]\n",
    "\n",
    "    # Clear the memory for unreferenced objects\n",
    "    gc.collect()\n",
    "\n",
    "    # Calculate the risk indicator\n",
    "    if total_word_count == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return matched_words_score / total_word_count\n",
    "\n",
    "def process_year(year, root, synonyms, labor_word_dict):\n",
    "    # Define the path to the transcripts based on the year\n",
    "    transcripts_path = os.path.join(root, str(year))\n",
    "    results = []\n",
    "\n",
    "    # Process each transcript file\n",
    "    for filename in os.listdir(transcripts_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(transcripts_path, filename)\n",
    "            Lrisk = compute_prisk_for_file(filepath, synonyms, labor_word_dict)\n",
    "            results.append((filename, Lrisk))\n",
    "            print(f\"Filename: {filename}\")\n",
    "            print(f\"Lrisk: {Lrisk}\")\n",
    "            print(\"--------\")\n",
    "\n",
    "    # Create a DataFrame from the results and save it as a CSV file\n",
    "    df = pd.DataFrame(results, columns=['Filename', 'Lrisk'])\n",
    "    df.to_csv(f\"Lrisk_{year}.csv\", index=False)\n",
    "    print(f\"Results for {year} saved to Lrisk_{year}.csv\")\n",
    "\n",
    "# Set the root directory for the transcripts\n",
    "root = 'E:\\\\workforce\\\\'\n",
    "\n",
    "# Process the transcripts for each year\n",
    "for year in range(2001, 2024):\n",
    "    process_year(year, root, synonyms, labor_word_dict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
